{
  "question_id" : 44805990,
  "title" : "Starting a large number of dependent process in async using python multiprocessing",
  "body" : "<p>Problem: I've a DAG(Directed-acyclic-graph) like structure for starting the execution of some massive data processing on a machine. Some of the process can only be started when their parent data processing is completed cause there is multi level of processing. I want to use python multiprocessing library to handle all on one single machine of it as first goal and later scale to execute on different machines using Managers. I've got no prior experience with python multiprocessing. Can anyone suggest if it's a good library to begin with? If yes, some basic implementation idea would do just fine. If not, what else can be used to do this thing in python?</p>\n\n<p>Example: </p>\n\n<p>A -> B</p>\n\n<p>B -> D, E, F, G</p>\n\n<p>C -> D</p>\n\n<p>In the above example i want to kick A &amp; C first(parallel), after their successful execution, other remaining processes would just wait for B to finish first. As soon as B finishes its execution all other process will start.</p>\n\n<p>P.S.: Sorry i cannot share actual data because confidential, though i tried to make it clear using the example.</p>\n",
  "link" : "https://stackoverflow.com/questions/44805990/starting-a-large-number-of-dependent-process-in-async-using-python-multiprocessi",
  "owner" : {
    "user_id" : 8226810,
    "user_type" : "registered",
    "display_name" : "Ethan Hunt",
    "profile_image" : "https://www.gravatar.com/avatar/6cc77537cc914ff7f387ad8eba58123d?s=128&d=identicon&r=PG&f=1",
    "link" : "https://stackoverflow.com/users/8226810/ethan-hunt",
    "reputation" : 1,
    "accept_rate" : null
  },
  "is_answered" : false,
  "creation_date" : 1498661640,
  "last_activity_date" : 1498661640,
  "tags" : [
    "python",
    "process",
    "multiprocessing",
    "python-multiprocessing"
  ],
  "score" : 0,
  "view_count" : 4,
  "answer_count" : 0
}