{
  "question_id" : 44806078,
  "title" : "How to use word embeddings/word2vec with complex physical objects",
  "body" : "<p>If my title is incorrect/could be better, please let me know.</p>\n\n<p>I've been trying to find an existing paper/article describing the problem that I'm having: I'm trying to create vectors for words so that they are equal to the sum of their parts.\nFor example: Cardinal(the bird) would be equal to the vectors of: red, bird, and ONLY that.\nIn order to train such a model, the input might be something like a dictionary, where each word is defined by it's attributes.\nSomething like: </p>\n\n<p>Cardinal: bird, red, ....</p>\n\n<p>Bluebird: blue, bird,....</p>\n\n<p>Bird: warm-blooded, wings, beak, two eyes, claws....</p>\n\n<p>Wings: Bone, feather....</p>\n\n<p>So in this instance, each word-vector is equal to the sum of the word-vector of its parts, and so on.</p>\n\n<p>I understand that in the original word2vec, semantic distance was preserved, such that Vec(Madrid)-Vec(Spain)+Vec(Paris) = approx Vec(Paris).</p>\n\n<p>Thanks!</p>\n\n<p>PS: Also, if it's possible, new words should be able to be added later on.</p>\n",
  "link" : "https://stackoverflow.com/questions/44806078/how-to-use-word-embeddings-word2vec-with-complex-physical-objects",
  "owner" : {
    "user_id" : 8226714,
    "user_type" : "registered",
    "display_name" : "Daniel Kaplan",
    "profile_image" : "https://www.gravatar.com/avatar/6da01ea29d4c5930eb23e25d5fdc020a?s=128&d=identicon&r=PG&f=1",
    "link" : "https://stackoverflow.com/users/8226714/daniel-kaplan",
    "reputation" : 1,
    "accept_rate" : null
  },
  "is_answered" : false,
  "creation_date" : 1498661829,
  "last_activity_date" : 1498661829,
  "tags" : [
    "nlp",
    "deep-learning",
    "word2vec"
  ],
  "score" : 0,
  "view_count" : 3,
  "answer_count" : 0
}